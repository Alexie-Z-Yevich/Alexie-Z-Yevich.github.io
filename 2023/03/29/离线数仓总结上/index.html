<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>离线数仓总结（上） | New Try &amp;&amp; New Life</title><meta name="author" content="Alexie-Z-Yevich"><meta name="copyright" content="Alexie-Z-Yevich"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="第一章 数据仓库的概念及架构选型数据仓库（Data Warehouse），是为企业制定决策，提供数据支持的。可以帮助企业，改进业务流程、提高产品质量等。 1、数据仓库的输入数据业务数据就是各行业在处理事务过程中产生的数据。比如用户在电商网站中登录、下单、支付等过程中，需要和网站后台数据库进行增删改查交互，产生的数据就是业务数据。业务数据通常存储在MySQL、Oracle等数据库中。 用户行为数据用">
<meta property="og:type" content="article">
<meta property="og:title" content="离线数仓总结（上）">
<meta property="og:url" content="https://www.fenrisx.icu/2023/03/29/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E6%80%BB%E7%BB%93%E4%B8%8A/index.html">
<meta property="og:site_name" content="New Try &amp;&amp; New Life">
<meta property="og:description" content="第一章 数据仓库的概念及架构选型数据仓库（Data Warehouse），是为企业制定决策，提供数据支持的。可以帮助企业，改进业务流程、提高产品质量等。 1、数据仓库的输入数据业务数据就是各行业在处理事务过程中产生的数据。比如用户在电商网站中登录、下单、支付等过程中，需要和网站后台数据库进行增删改查交互，产生的数据就是业务数据。业务数据通常存储在MySQL、Oracle等数据库中。 用户行为数据用">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.fenrisx.icu/images/head.jpg">
<meta property="article:published_time" content="2023-03-29T14:21:03.000Z">
<meta property="article:modified_time" content="2023-03-30T13:30:22.251Z">
<meta property="article:author" content="Alexie-Z-Yevich">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="大数据组件">
<meta property="article:tag" content="数据仓库">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.fenrisx.icu/images/head.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://www.fenrisx.icu/2023/03/29/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E6%80%BB%E7%BB%93%E4%B8%8A/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Alexie-Z-Yevich","link":"链接: ","source":"来源: New Try && New Life","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '离线数仓总结（上）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-30 21:30:22'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">36</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/attacks/"><i class="fa-fw fas fa-images"></i><span> 哒咩</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="New Try &amp;&amp; New Life"><span class="site-name">New Try &amp;&amp; New Life</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/attacks/"><i class="fa-fw fas fa-images"></i><span> 哒咩</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">离线数仓总结（上）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-29T14:21:03.000Z" title="发表于 2023-03-29 22:21:03">2023-03-29</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-03-30T13:30:22.251Z" title="更新于 2023-03-30 21:30:22">2023-03-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Little-Tips/">Little Tips</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="离线数仓总结（上）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="第一章-数据仓库的概念及架构选型"><a href="#第一章-数据仓库的概念及架构选型" class="headerlink" title="第一章 数据仓库的概念及架构选型"></a>第一章 数据仓库的概念及架构选型</h2><p>数据仓库（Data Warehouse），是为企业制定决策，提供数据支持的。可以帮助企业，改进业务流程、提高产品质量等。</p>
<h4 id="1、数据仓库的输入数据"><a href="#1、数据仓库的输入数据" class="headerlink" title="1、数据仓库的输入数据"></a>1、数据仓库的输入数据</h4><h6 id="业务数据"><a href="#业务数据" class="headerlink" title="业务数据"></a>业务数据</h6><p>就是各行业在处理事务过程中产生的数据。比如用户在电商网站中登录、下单、支付等过程中，需要和网站后台数据库进行增删改查交互，产生的数据就是业务数据。业务数据通常存储在MySQL、Oracle等数据库中。</p>
<h6 id="用户行为数据"><a href="#用户行为数据" class="headerlink" title="用户行为数据"></a>用户行为数据</h6><p>用户在使用产品过程中，通过埋点收集与客户端产品交互过程中产生的数据，并发往日志服务器进行保存。比如页面浏览、点击、停留、评论、点赞、收藏等。用户行为数据通常存储在日志文件中。</p>
<h6 id="爬虫数据"><a href="#爬虫数据" class="headerlink" title="爬虫数据"></a>爬虫数据</h6><p>通常是通过技术手段获取其他公司网站的数据。不建议同学们这样去做。</p>
<blockquote>
<p>通常来说，业务数据就是后端持久化到数据库的日志记录，例如Mysql的Binlog日志；用户行为数据主要是前端进行埋点采集到的数据，嗯，简单让new bing来介绍下采集流程吧：</p>
<p>前端埋点到大数据采集的完整流程包括数据采集层、数据接入层、数据处理层、数据应用层这四个层次。在数据采集层，可以通过传统的埋点方式，在需要上报的位置组织数据、调用API、将数据传给后端，也可以通过“无埋点”概念，在底层hook所有的点击事件，自动采集全部事件并上报埋点数据。在数据接入层，可以通过Flume、Kafka等工具将数据接入到Hadoop生态圈中。在数据处理层，可以使用Hive、Spark SQL等工具进行数据清洗、转换和计算。在数据应用层，可以使用Tableau、Echarts等工具进行可视化展示。</p>
<p>(1) 一个埋点的求生之路——数据处理全流程解析 - 知乎. <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/133090030">https://zhuanlan.zhihu.com/p/133090030</a></p>
<p>(2) 数据采集：埋点、采集、存储及分析 - Hider1214 - 博客园. <a target="_blank" rel="noopener" href="https://www.cnblogs.com/hider/p/13967167.html">https://www.cnblogs.com/hider/p/13967167.html</a></p>
<p>(3) 前端埋点简单实现方式 - 掘金. <a target="_blank" rel="noopener" href="https://juejin.cn/post/7047710777507053582">https://juejin.cn/post/7047710777507053582</a></p>
</blockquote>
<h4 id="2、项目框架"><a href="#2、项目框架" class="headerlink" title="2、项目框架"></a>2、项目框架</h4><p>关于选用版本的详细解释，组件详细介绍单开一栏说明。</p>
<table>
<thead>
<tr>
<th>框架</th>
<th>版本</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>jdk</td>
<td>1.8</td>
<td>Java版本，Hadoop体系建议使用1.8，虽然兼容Java11但是Hive除了最新的4.0.0alpha版本以外，其他的都只支持到1.8。Java11以上不支持。</td>
</tr>
<tr>
<td>Hadoop</td>
<td>3.1.3</td>
<td>选用视频配套版本，选择太旧的2.x可能出现依赖缺失，最新的3.4.3则存在依赖改名的问题</td>
</tr>
<tr>
<td>Zookeeper</td>
<td>3.5.7</td>
<td></td>
</tr>
<tr>
<td>Mysql</td>
<td>5.7.16</td>
<td>可以选用8系列，但是不能低于5.7版本，5.7.8之后支持Json，同时对InnoDB和一些强政策进行了更改。详情可看<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29726382">数据库MySQL 5.7版本介绍</a></td>
</tr>
<tr>
<td>Hive</td>
<td>3.1.2</td>
<td>Hive和Spark以及Hadoop版本要互相兼容，采用自编译等方式可以自定义版本</td>
</tr>
<tr>
<td>Flume</td>
<td>1.9.0</td>
<td>不建议选用2.0.0以上版本，配置文件有点改动（大概是.properties-&gt;.xml）</td>
</tr>
<tr>
<td>Kafka</td>
<td>3.0.0</td>
<td></td>
</tr>
<tr>
<td>Spark</td>
<td>3.0.0</td>
<td></td>
</tr>
<tr>
<td>DataX</td>
<td>3.0.0</td>
<td></td>
</tr>
<tr>
<td>Superset</td>
<td><strong>1.4.2</strong></td>
<td>推荐版本很搞，更新之后很多依赖都没了，不建议使用2.0.0，如果需要2.0.0+可以使用网上的一键部署版本，不然自己调整配置会很麻烦（需要一定conda、python知识）</td>
</tr>
<tr>
<td>DolphinScheduler</td>
<td>2.0.3</td>
<td></td>
</tr>
<tr>
<td>Maxwell</td>
<td>1.29.2</td>
<td></td>
</tr>
</tbody></table>
<h4 id="3、测试集群服务器规划"><a href="#3、测试集群服务器规划" class="headerlink" title="3、测试集群服务器规划"></a>3、测试集群服务器规划</h4><table>
<thead>
<tr>
<th>服务名称</th>
<th>子服务</th>
<th>服务器hadoop102</th>
<th>服务器hadoop1</th>
<th>服务器hadoop104</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS</td>
<td>NameNode</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>DataNode</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>SecondaryNameNode</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>Yarn</td>
<td>NodeManager</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>Resourcemanager</td>
<td></td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>Zookeeper</td>
<td>Zookeeper Server</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>Flume（采集日志）</td>
<td>Flume</td>
<td>√</td>
<td>√</td>
<td></td>
</tr>
<tr>
<td>Kafka</td>
<td>Kafka</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>Flume（消费Kafka日志）</td>
<td>Flume</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>Flume（消费Kafka业务）</td>
<td>Flume</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr>
<td>Hive</td>
<td></td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>MySQL</td>
<td>MySQL</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr>
<td>DataX</td>
<td></td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>Spark</td>
<td></td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>DolphinScheduler</td>
<td>ApiApplicationServer</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>AlertServer</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>MasterServer</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td>WorkerServer</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td></td>
<td>LoggerServer</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr>
<td>Superset</td>
<td>Superset</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr>
<td>服务数总计</td>
<td></td>
<td>16</td>
<td>11</td>
<td>12</td>
</tr>
</tbody></table>
<hr>
<h2 id="第二章-组件及常见问题"><a href="#第二章-组件及常见问题" class="headerlink" title="第二章 组件及常见问题"></a>第二章 组件及常见问题</h2><h4 id="0、虚拟机"><a href="#0、虚拟机" class="headerlink" title="0、虚拟机"></a>0、虚拟机</h4><p>能上哪个版本的VMware就上那个版本的VMware，虚拟机版本选用对环境影响并不大，不同Linux发行版之间基本是配置相同，包管理工具不同罢了，值得注意的是：选用CentOS Stream 9的配置项会和CentOS7之类的差别较大，在配置CentOS Stream 9作为虚拟机的时候可以参考以下文章：<a target="_blank" rel="noopener" href="https://www.hxstrive.com/article/1053.htm">CentOS Stream9 设置静态IP</a>。</p>
<p><strong>！！注意一下内存分配</strong>：HiveServer2启动查询功能大概率是将结果暂存在内存中，开8G很容易OOM，而DolphinScheduler更加离谱，全节点启动的情况下，8G内存甚至任务都无法执行，推荐配置16G、8G、8G，最低配置如下：</p>
<p>Hadoop102：8G+（我配的10G，结束时约空闲1~2G）</p>
<p>Hadoop103&#x2F;Hadoop104：6G+（我配的8G，结束时约空闲1~3G）</p>
<h4 id="0-1、Jdk"><a href="#0-1、Jdk" class="headerlink" title="0.1、Jdk"></a>0.1、Jdk</h4><p>版本选择1.8，以所有组件中要求版本最高的那个作为适配版本，所有组件中Hive只能在1.8中启动，故选择1.8版本；当然如果想采用最新版本体验或是仍然想用更高级Java版本中的语法糖，那么可以安装多个Java，让Hive执行其他Java即可。</p>
<h4 id="0-2、Mock模拟数据准备"><a href="#0-2、Mock模拟数据准备" class="headerlink" title="0.2、Mock模拟数据准备"></a>0.2、Mock模拟数据准备</h4><h6 id="（1）生成日志数据"><a href="#（1）生成日志数据" class="headerlink" title="（1）生成日志数据"></a>（1）生成日志数据</h6><p>这里使用文档中准备的Java程序执行即可，使用基本流程是：</p>
<ul>
<li>依次启动Zookeeper、Kafka、Flume（采集日志）；</li>
<li>执行Java程序，在选定目录下生成Json日志；</li>
<li>Flume将生成的日志数据采集到HDFS上。</li>
</ul>
<h6 id="（2）生成业务数据"><a href="#（2）生成业务数据" class="headerlink" title="（2）生成业务数据"></a>（2）生成业务数据</h6><p>这里使用文档中准备的Java程序执行即可，使用基本流程是：</p>
<ul>
<li>修改application.properties中的mock.clear和mock.clear.user数据</li>
<li>执行Java程序，在MySQL数据库中生成业务数据</li>
</ul>
<h4 id="0-3、Source环境总览"><a href="#0-3、Source环境总览" class="headerlink" title="0.3、Source环境总览"></a>0.3、Source环境总览</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HADOOP_HOME</span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">KAFKA_HOME</span></span><br><span class="line">export KAFKA_HOME=/opt/module/kafka</span><br><span class="line">export PATH=$PATH:$KAFKA_HOME/bin</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">HIVE_HOME</span></span><br><span class="line">export HIVE_HOME=/opt/module/hive</span><br><span class="line">export PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">SPARK_HOME</span></span><br><span class="line">export SPARK_HOME=/opt/module/spark</span><br><span class="line">export PATH=$PATH:$SPARK_HOME/bin</span><br></pre></td></tr></table></figure>



<h4 id="1、Hadoop"><a href="#1、Hadoop" class="headerlink" title="1、Hadoop"></a>1、Hadoop</h4><p>Hadoop是一个由Apache基金会所开发的分布式系统基础架构，是一个存储系统 + 计算框架的软件框架。主要解决海量数据存储与计算的问题，是大数据技术中的基石。Hadoop以一种可靠、高效、可伸缩的方式进行数据处理，用户可以在不了解分布式底层细节的情况下，开发出分布式程序。</p>
<h6 id="（1）配置流程："><a href="#（1）配置流程：" class="headerlink" title="（1）配置流程："></a>（1）配置流程：</h6><ul>
<li><p>核心配置文件core-site.xml</p>
<ul>
<li>配置NameNode地址</li>
<li>指定Hadoop数据存储目录</li>
<li>配置HDFS网页登录使用的静态用户</li>
<li>配置用户允许通过代理访问的主机节点</li>
<li>配置用户允许通过代理用户所属组</li>
<li>配置用户允许通过代理的用户</li>
</ul>
</li>
<li><p>HDFS配置文件hdfs-site.xml</p>
<ul>
<li>配置NameNode Web端访问端口</li>
<li>配置Secondary NameNode Web端访问端口</li>
<li>指定HDFS副本的数量</li>
</ul>
</li>
<li><p>YARN配置文件yarn-site.xml</p>
<ul>
<li>指定MapReduce走shuffle</li>
<li>指定ResourceManager地址</li>
<li>配置环境变量继承</li>
<li>配置YARN单个容器允许分配的最大最小内存</li>
<li>配置YARN容器允许管理的物理内存大小</li>
<li>关闭YARN对物理内存和虚拟内存的限制检查</li>
<li>开启日志聚集功能</li>
<li>设置日志聚集服务器地址</li>
<li>设置日志保留时间为7天</li>
</ul>
</li>
<li><p>MapReduce配置文件mapred-site.xml</p>
<ul>
<li>指定MapReduce程序运行在YARN上</li>
<li>配置历史服务器端地址</li>
<li>配置历史服务器Web端地址</li>
</ul>
</li>
<li><p>配置workers（集群）</p>
</li>
<li><p>分发到所有集群机器上</p>
</li>
<li><p>初始化NameNode节点</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<h6 id="（2）常见启停命令"><a href="#（2）常见启停命令" class="headerlink" title="（2）常见启停命令"></a>（2）常见启停命令</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh                        # 启动HDFS</span><br><span class="line">start-yarn.sh                       # 启动YARN</span><br><span class="line">mapred --daemon start historyserver # 启动历史日志采集</span><br></pre></td></tr></table></figure>

<h6 id="（3）参数调优"><a href="#（3）参数调优" class="headerlink" title="（3）参数调优"></a>（3）参数调优</h6><ul>
<li><p>hdfs-site.xml</p>
<ul>
<li>NameNode有一个工作线程池，用来处理不同DataNode的并发心跳以及客户端并发的元数据操作。对于大集群或者有大量客户端的集群来说，通常需要增大参数dfs.namenode.handler.count的默认值10。</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;10&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>  $$<br>  dfs.namenode.handler.count&#x3D;20 × log_e^ClusterSize， 比如集群规模为8台时，此参数设置为41。<br>  $$</p>
<ul>
<li><p>yarn-site.xml</p>
<ul>
<li><p>数据统计主要用HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启的JVM重用，而且IO没有阻塞，内存用了不到50%。但是还是跑的非常慢，而且数据量洪峰过来时，整个集群都会宕掉。</p>
</li>
<li><p>内存利用率不够。这个一般是Yarn的2个配置造成的，单个任务可以申请的最大内存大小，和Hadoop单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">（a）yarn.nodemanager.resource.memory-mb</span><br><span class="line">表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB，则需要调减小这个值，而YARN不会智能的探测节点的物理内存总量。</span><br><span class="line">（b）yarn.scheduler.maximum-allocation-mb</span><br><span class="line">单个任务可申请的最多物理内存量，默认是8192（MB）。</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h4 id="2、Zookeeper"><a href="#2、Zookeeper" class="headerlink" title="2、Zookeeper"></a>2、Zookeeper</h4><p>ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，它包含一个简单的原语集，分布式应用程序可以基于它实现同步服务，配置维护和命名服务等。ZooKeeper作为一个分布式的服务框架，主要用来解决分布式集群中应用系统的一致性问题，它能提供基于类似于文件系统的目录节点树方式的数据存储，Zookeeper作用主要是用来维护和监控存储的数据的状态变化，通过监控这些数据状态的变化，从而达到基于数据的集群管理。</p>
<h6 id="（1）配置流程"><a href="#（1）配置流程" class="headerlink" title="（1）配置流程"></a>（1）配置流程</h6><ul>
<li><p>配置myid文件，每个服务器上的myid是唯一编号，代表单独的Zookeeper server。</p>
</li>
<li><p>配置*&#x2F;conf目录下的zoo_sample.cfg文件</p>
<ul>
<li><p>改名，去掉sample启用配置</p>
</li>
<li><p>修改数据存储路径，配置到myid对应的目录</p>
</li>
<li><p>增加server.*配置。例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.2=hadoop102:2888:3888 # .2就是myid中的server编号</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>同步配置到其他服务器（一定记得修改myid）</p>
</li>
</ul>
<h6 id="（2）常见启停脚本"><a href="#（2）常见启停脚本" class="headerlink" title="（2）常见启停脚本"></a>（2）常见启停脚本</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start  # 启动Zookeeper</span><br><span class="line">zkServer.sh status # 查看Zookeeper状态</span><br><span class="line">zkServer.sh stop   # 停止Zookeeper进程</span><br></pre></td></tr></table></figure>



<h4 id="3、Kafka"><a href="#3、Kafka" class="headerlink" title="3、Kafka"></a>3、Kafka</h4><p>Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者在网站中的所有动作流数据。Kafka最初由LinkedIn公司发布，使用Scala语言编写，与2010年12月份开源，成为Apache的顶级子项目。</p>
<p>Kafka是一种高吞吐量、持久性、分布式的发布订阅的消息队列系统。它可以在廉价的PC Server上搭建起大规模消息系统。</p>
<h6 id="（1）配置流程-1"><a href="#（1）配置流程-1" class="headerlink" title="（1）配置流程"></a>（1）配置流程</h6><ul>
<li><p>配置*&#x2F;config&#x2F;目录下的server.properties文件</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#broker的全局唯一编号，不能重复，只能是数字。每个服务器上的数字不能一样。</span><br><span class="line">broker.id=0</span><br><span class="line">#kafka运行日志(数据)存放的路径，路径不需要提前创建，kafka自动帮你创建，可以配置多个磁盘路径，路径与路径之间可以用&quot;,&quot;分隔</span><br><span class="line">log.dirs=/opt/module/kafka/datas</span><br><span class="line">#配置连接Zookeeper集群地址（在zk根目录下创建/kafka，方便管理）</span><br><span class="line">zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</span><br></pre></td></tr></table></figure>
</li>
<li><p>分发软件包，记得修改broker.id</p>
</li>
</ul>
<h6 id="（2）常见启停脚本-1"><a href="#（2）常见启停脚本-1" class="headerlink" title="（2）常见启停脚本"></a>（2）常见启停脚本</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">配置过zookeeper.connect之后需要先启动Zookeeper服务</span></span><br><span class="line">kafka-server-start.sh -daemon config/server.properties  # 根据配置文件启动Kafka</span><br><span class="line">kafka-server-stop.sh                                    # 关闭Kafka</span><br></pre></td></tr></table></figure>

<h6 id="（3）常见命令行操作"><a href="#（3）常见命令行操作" class="headerlink" title="（3）常见命令行操作"></a>（3）常见命令行操作</h6><ul>
<li><p>主题命令操作</p>
<ul>
<li><p>Kafka中的数据单元是消息，可以把它当作数据库里的一行“数据”或者一条“记录”来理解。Kafka通过主题来进行分类，主题就好比数据库中的表，每个主题包含多个分区，分区可以分布在不同的服务器上，也就是说通过这种方式来实现分布式数据的存储和读取。</p>
<p>主题是Kafka中的基础概念，是一切消息处理的基础。主题属于Kafka元数据的一部分，会存储在Zookeeper中。生产者发布消息到某一特定主题上，由消费者去消费特定主题的消息。消费者可以订阅一个或多个主题。</p>
</li>
</ul>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh  # 查看操作命令主题</span><br></pre></td></tr></table></figure></li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的Kafka Broker主机名称和端口号。</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的topic名称。</td>
</tr>
<tr>
<td>–create</td>
<td>创建主题。</td>
</tr>
<tr>
<td>–delete</td>
<td>删除主题。</td>
</tr>
<tr>
<td>–alter</td>
<td>修改主题。</td>
</tr>
<tr>
<td>–list</td>
<td>查看所有主题。</td>
</tr>
<tr>
<td>–describe</td>
<td>查看主题详细描述。</td>
</tr>
<tr>
<td>–partitions &lt;Integer: # of partitions&gt;</td>
<td>设置分区数。</td>
</tr>
<tr>
<td>–replication-factor&lt;Integer: replication factor&gt;</td>
<td>设置分区副本。</td>
</tr>
<tr>
<td>–config &lt;String: name&#x3D;value&gt;</td>
<td>更新系统默认的配置。</td>
</tr>
</tbody></table>
<ul>
<li><p>生产者命令操作</p>
<ul>
<li>Kafka生产者是指向Kafka集群发送消息的客户端应用程序。生产者将消息封装成一个ProducerRecord向Kafka集群中的某个主题发送消息。发送的消息首先会经过序列化器进行序列化，以便在网络中传输。发送的消息需要经过分区器来决定该消息会分发到主题对应的分区，当然如果指定了分区，那么就不需要分区器来决定。</li>
</ul>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-producer.sh  # 查看操作生产者</span><br></pre></td></tr></table></figure></li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的Kafka Broker主机名称和端口号。</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的topic名称。</td>
</tr>
</tbody></table>
<ul>
<li><p>消费者命令操作</p>
<ul>
<li>Kafka消费者是指从Kafka集群中读取消息的客户端应用程序。消费者将从一个或多个主题订阅消息，并在消费者组中进行组织。当多个消费者形成一个消费组来消费主题时，每个消费者会收到不同分区的消息。消费者可以控制消息的读取位置，从而实现对消息的重复读取或跳过。</li>
</ul>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh  # 查看操作消费者</span><br></pre></td></tr></table></figure></li>
</ul>
<table>
<thead>
<tr>
<th>参数</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>–bootstrap-server &lt;String: server toconnect to&gt;</td>
<td>连接的Kafka Broker主机名称和端口号。</td>
</tr>
<tr>
<td>–topic &lt;String: topic&gt;</td>
<td>操作的topic名称。</td>
</tr>
<tr>
<td>–from-beginning</td>
<td>从头开始消费。</td>
</tr>
<tr>
<td>–group &lt;String: consumer group id&gt;</td>
<td>指定消费者组名称。</td>
</tr>
</tbody></table>
<h4 id="4、Flume"><a href="#4、Flume" class="headerlink" title="4、Flume"></a>4、Flume</h4><p>Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。它的主要作用是将数据从各种数据源（如Web服务器）收集到Hadoop的HDFS中，或者将数据从一个地方传输到另一个地方。Flume的核心概念包括：Source、Channel和Sink。Source是数据源，可以是Avro、Thrift、JMS、Netcat、Exec等；Channel是缓存区，用于存储Source产生的数据；Sink是数据目的地，可以是HDFS、HBase、Solr、Elasticsearch等。</p>
<ul>
<li>删除lib目录下的guava.jar以兼容Hadoop</li>
</ul>
<h6 id="（1）整体流程"><a href="#（1）整体流程" class="headerlink" title="（1）整体流程"></a>（1）整体流程</h6><p>日志采集Flume需要采集日志文件内容，并对日志格式（JSON）进行校验，然后将校验通过的日志发送到Kafka。此处可选择TaildirSource和KafkaChannel，并配置日志校验拦截器。</p>
<ul>
<li>TailDirSource相比ExecSource、SpoolingDirectorySource的优势<ul>
<li>TailDirSource：断点续传、多目录。Flume1.6以前需要自己自定义Source记录每次读取文件位置，实现断点续传。</li>
<li>ExecSource可以实时搜集数据，但是在Flume不运行或者Shell命令出错的情况下，数据将会丢失。</li>
<li>SpoolingDirectorySource监控目录，支持断点续传。</li>
</ul>
</li>
<li>Kafka Channel<ul>
<li>采用Kafka Channel，省去了Sink，提高了效率。</li>
</ul>
</li>
</ul>
<blockquote>
<p>Flume执行job实现日志的采集，因此无需就行过多的原生配置，只需要针对每个任务写对应的job即可。</p>
</blockquote>
<h6 id="（2）配置实操"><a href="#（2）配置实操" class="headerlink" title="（2）配置实操"></a>（2）配置实操</h6><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># job/file_to_kafka.conf</span><br><span class="line"># 定义组件</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># 配置source</span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*</span><br><span class="line">a1.sources.r1.positionFile = /opt/module/flume/taildir_position.json</span><br><span class="line">a1.sources.r1.interceptors =  i1</span><br><span class="line">a1.sources.r1.interceptors.i1.type = xxx.ETLInterceptor$Builder  # 自定义拦截器jar包</span><br><span class="line"></span><br><span class="line"># 配置channel</span><br><span class="line">a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel</span><br><span class="line">a1.channels.c1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092</span><br><span class="line">a1.channels.c1.kafka.topic = topic_log</span><br><span class="line">a1.channels.c1.parseAsFlumeEvent = false</span><br><span class="line"></span><br><span class="line"># 组装 </span><br><span class="line">a1.sources.r1.channels = c1</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flume-ng agent -n a1 -c conf/ -f job/file_to_kafka.conf -Dflume.root.logger=info,console</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">这段代码是用于启动Flume的命令行。其中，`flume-ng`是启动Flume的命令，`agent`表示启动的是一个agent，`-n a1`表示agent的名称为a1，`-c conf/`表示配置文件所在的目录为conf，`-f job/file_to_kafka.conf`表示使用名为file_to_kafka.conf的配置文件，`-Dflume.root.logger=info,console`表示设置日志级别为info，并将日志输出到控制台。</span></span><br></pre></td></tr></table></figure>



<h4 id="5、MySQL"><a href="#5、MySQL" class="headerlink" title="5、MySQL"></a>5、MySQL</h4><h6 id="配置流程"><a href="#配置流程" class="headerlink" title="配置流程"></a>配置流程</h6><ul>
<li>安装MySQL依赖</li>
<li>安装mysql-client</li>
<li>安装mysql-server</li>
<li>配置MySQL<ul>
<li>设置复杂密码</li>
<li>更改MySQL密码策略</li>
<li>设置简单密码</li>
</ul>
</li>
</ul>
<h4 id="6、Maxwell"><a href="#6、Maxwell" class="headerlink" title="6、Maxwell"></a>6、Maxwell</h4><p>Maxwell是一个能实时读取MySQL二进制日志binlog，并生成 JSON 格式的消息，作为生产者发送给 Kafka、Kinesis、RabbitMQ、Redis、Google Cloud Pub&#x2F;Sub、文件或其它平台的应用程序³。它的主要作用是将MySQL数据库中的数据变更事件转换成JSON格式的消息，以便于在分布式系统中进行数据同步。Maxwell可以通过配置文件来指定需要同步的表，以及需要过滤掉的字段等信息。</p>
<p><strong>原理：</strong>将自己伪装成slave，并遵循MySQL主从复制的协议，从master同步数据。</p>
<h6 id="（1）配置流程-2"><a href="#（1）配置流程-2" class="headerlink" title="（1）配置流程"></a>（1）配置流程</h6><ul>
<li><p>修改MySQL配置，启用Binglog日志（默认不开启）后重启MySQL服务。</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line"></span><br><span class="line">#数据库id</span><br><span class="line">server-id = 1</span><br><span class="line">#启动binlog，该参数的值会作为binlog的文件名</span><br><span class="line">log-bin=mysql-bin</span><br><span class="line">#binlog类型，maxwell要求为row类型</span><br><span class="line">binlog_format=row</span><br><span class="line">#启用binlog的数据库，需根据实际情况作出修改</span><br><span class="line">binlog-do-db=gmall</span><br></pre></td></tr></table></figure>

<ul>
<li><p>几种常见的Binlog模式</p>
<ul>
<li><p><strong>Statement-based：</strong>基于语句，Binlog会记录所有写操作的SQL语句，包括insert、update、delete等。</p>
<p>优点： 节省空间</p>
<p>缺点： 有可能造成数据不一致，例如insert语句中包含now()函数。</p>
</li>
<li><p><strong>Row-based：</strong>基于行，Binlog会记录每次写操作后被操作行记录的变化。</p>
<p>优点：保持数据的绝对一致性。</p>
<p>缺点：占用较大空间。</p>
</li>
<li><p><strong>mixed：</strong>混合模式，默认是Statement-based，如果SQL语句可能导致数据不一致，就自动切换到Row-based。</p>
</li>
</ul>
</li>
<li><p><strong>Maxwell要求Binlog采用Row-based模式。</strong></p>
</li>
</ul>
</li>
<li><p>配置maxwell目录下的config.properties.example文件（元数据库、生产者、集群配置）</p>
<ul>
<li>重命名为config.properties</li>
</ul>
  <figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Maxwell数据发送目的地，可选配置有stdout|file|kafka|kinesis|pubsub|sqs|rabbitmq|redis</span></span><br><span class="line"><span class="attr">producer</span>=<span class="string">kafka</span></span><br><span class="line"><span class="comment">#目标Kafka集群地址</span></span><br><span class="line"><span class="attr">kafka.bootstrap.servers</span>=<span class="string">hadoop102:9092,hadoop103:9092</span></span><br><span class="line"><span class="comment">#目标Kafka topic，可静态配置，例如:maxwell，也可动态配置，例如：%&#123;database&#125;_%&#123;table&#125;</span></span><br><span class="line"><span class="attr">kafka_topic</span>=<span class="string">maxwell</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">#MySQL相关配置</span></span><br><span class="line"><span class="attr">host</span>=<span class="string">hadoop102</span></span><br><span class="line"><span class="attr">user</span>=<span class="string">maxwell</span></span><br><span class="line"><span class="attr">password</span>=<span class="string">maxwell</span></span><br><span class="line"><span class="attr">jdbc_options</span>=<span class="string">useSSL=false&amp;serverTimezone=Asia/Shanghai</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="（2）常见启停脚本-2"><a href="#（2）常见启停脚本-2" class="headerlink" title="（2）常见启停脚本"></a>（2）常见启停脚本</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">maxwell --config /opt/module/maxwell/config.properties --daemon                         # 启动maxwell</span><br><span class="line">ps -ef | grep maxwell | grep -v grep | grep maxwell | awk &#x27;&#123;print $2&#125;&#x27; | xargs kill -9  # 停止maxwell</span><br></pre></td></tr></table></figure>

<h6 id="（3）增量同步和全量同步"><a href="#（3）增量同步和全量同步" class="headerlink" title="（3）增量同步和全量同步"></a>（3）增量同步和全量同步</h6><ul>
<li><p>增量同步：启用Kafka消费者、maxwell，Mock业务数据即可在Kafka中观察到生成的数据；</p>
</li>
<li><p>全量同步：</p>
<ul>
<li>Maxwell提供了bootstrap功能来进行历史数据的全量同步，输出结果为Json格式</li>
</ul>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">maxwell-bootstrap --database gmall --table user_info --config /opt/module/maxwell/config.properties</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">这段代码是在运行maxwell-bootstrap命令，它会将gmall数据库中的user_info表的数据导入到另一个数据库中。其中，--config /opt/module/maxwell/config.properties是指定了配置文件的路径。</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="7、DataX"><a href="#7、DataX" class="headerlink" title="7、DataX"></a>7、DataX</h4><p>DataX 是阿里巴巴开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。源码地址：<a target="_blank" rel="noopener" href="https://github.com/alibaba/DataX">https://github.com/alibaba/DataX</a></p>
<p>为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。</p>
<h6 id="（1）配置流程-3"><a href="#（1）配置流程-3" class="headerlink" title="（1）配置流程"></a>（1）配置流程</h6><ul>
<li><p>解压自检</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">python /opt/module/datax/bin/datax.py /opt/module/datax/job/job.json</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">出现如下反馈代表安装成功</span></span><br><span class="line">……</span><br><span class="line">2021-10-12 21:51:12.335 [job-0] INFO  JobContainer - </span><br><span class="line">任务启动时刻                    : 2021-10-12 21:51:02</span><br><span class="line">任务结束时刻                    : 2021-10-12 21:51:12</span><br><span class="line">任务总计耗时                    :                 10s</span><br><span class="line">任务平均流量                    :          253.91KB/s</span><br><span class="line">记录写入速度                    :          10000rec/s</span><br><span class="line">读出记录总数                    :              100000</span><br><span class="line">读写失败总数                    :                   0</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="（2）配置说明"><a href="#（2）配置说明" class="headerlink" title="（2）配置说明"></a>（2）配置说明</h6><p>配置文件模板为json，json最外层是一个job，job包含setting和content两部分，其中setting用于对整个job进行配置，content用户配置数据源和目的地。</p>
<p><img src="/2023/03/29/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E6%80%BB%E7%BB%93%E4%B8%8A/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E6%80%BB%E7%BB%93%EF%BC%88%E4%B8%8A%EF%BC%89/1.png"></p>
<h6 id="（3）DataX优化"><a href="#（3）DataX优化" class="headerlink" title="（3）DataX优化"></a>（3）DataX优化</h6><p>DataX3.0提供了包括通道(并发)、记录流、字节流三种流控模式，可以随意控制你的作业速度，让你的作业在数据库可以承受的范围内达到最佳的同步速度。</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>job.setting.speed.channel</td>
<td>并发数</td>
</tr>
<tr>
<td>job.setting.speed.record</td>
<td>总record限速</td>
</tr>
<tr>
<td>job.setting.speed.byte</td>
<td>总byte限速</td>
</tr>
<tr>
<td>core.transport.channel.speed.record</td>
<td>单个channel的record限速，默认值为10000（10000条&#x2F;s）</td>
</tr>
<tr>
<td>core.transport.channel.speed.byte</td>
<td>单个channel的byte限速，默认值1024*1024（1M&#x2F;s）</td>
</tr>
</tbody></table>
<p><strong>注意事项：</strong></p>
<p>1.若配置了总record限速，则必须配置单个channel的record限速</p>
<p>2.若配置了总byte限速，则必须配置单个channe的byte限速</p>
<p>3.若配置了总record限速和总byte限速，channel并发数参数就会失效。因为配置了总record限速和总byte限速之后，实际channel并发数是通过计算得到的：</p>
<p><strong>计算公式为:</strong></p>
<p>min(总byte限速&#x2F;单个channel的byte限速，总record限速&#x2F;单个channel的record限速)</p>
<h4 id="8、Hive"><a href="#8、Hive" class="headerlink" title="8、Hive"></a>8、Hive</h4><p>Hive是一个基于Hadoop的数据仓库工具，用于处理结构化数据。它可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。Hive的操作接口采用类SQL语法，提供快速开发的能力，避免了写MapReduce，减少开发人员的学习成本，功能扩展很方便。</p>
<p>你可以使用Hive来进行数据提取、转化、加载，这是一种可以存储、查询和分析存储在Hadoop中的大规模数据的机制。你可以在Hive中使用类SQL语句来查询数据，这些语句会被转换成MapReduce任务来执行。你需要安装Hive并配置它以便使用它。你可以在网上找到很多关于如何使用Hive的教程和指南。</p>
<h6 id="配置流程-1"><a href="#配置流程-1" class="headerlink" title="配置流程"></a>配置流程</h6><ul>
<li><p>解决日志包冲突，删除lib下的log4j-slf4j-impl.jar包</p>
</li>
<li><p>拷贝对应版本的mysql驱动到lib目录 mysql-connector-java-5.1.27-bin.jar</p>
</li>
<li><p>新建hive-site.xml</p>
  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=<span class="string">&quot;text/xsl&quot;</span> href=<span class="string">&quot;configuration.xsl&quot;</span>?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://hadoop102:3306/metastore?useSSL=false<span class="symbol">&amp;amp;</span>useUnicode=true<span class="symbol">&amp;amp;</span>characterEncoding=UTF-8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.header<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.cli.print.current.db<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在MySQL创建Hive元数据库metastore</p>
</li>
<li><p>初始化Hive元数据库</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">schematool <span class="operator">-</span>initSchema <span class="operator">-</span>dbType mysql <span class="operator">-</span>verbose</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改元数据库字符集</p>
<ul>
<li>Hive元数据库的字符集默认为Latin1，由于其不支持中文字符，故若建表语句中包含中文注释，会出现乱码现象。修改Hive元数据库中存储注释的字段的字符集为utf-8。</li>
</ul>
  <figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> COLUMNS_V2 modify <span class="keyword">column</span> COMMENT <span class="type">varchar</span>(<span class="number">256</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> TABLE_PARAMS modify <span class="keyword">column</span> PARAM_VALUE mediumtext <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="9、Spark"><a href="#9、Spark" class="headerlink" title="9、Spark"></a>9、Spark</h4><p>Spark是一个快速的、通用的、基于内存的分布式计算系统，用于大规模数据处理。它是UC Berkeley AMP lab所开源的类Hadoop MapReduce的通用并行框架，拥有Hadoop MapReduce所具有的优点；但不同于MapReduce的是Job中间输出和结果可以保存在内存中，从而不再需要读写HDFS，因此Spark能更好地适用于数据挖掘与机器学习等需要迭代的MapReduce的算法。</p>
<h6 id="（1）兼容性说明"><a href="#（1）兼容性说明" class="headerlink" title="（1）兼容性说明"></a>（1）兼容性说明</h6><p>官网下载的Hive3.1.2和Spark3.0.0默认是不兼容的。因为Hive3.1.2支持的Spark版本是2.4.5，所以需要我们重新编译Hive3.1.2版本。</p>
<p>编译步骤：官网下载Hive3.1.2源码，修改pom文件中引用的Spark版本为3.0.0，如果编译通过，直接打包获取jar包。如果报错，就根据提示，修改相关方法，直到不报错，打包获取jar包。</p>
<blockquote>
<p>这里我试过了hive4.0.0alpha去兼容jdk11，但是无疾而终（默认启动的是beeline不是hive，这个时候我还没学hive，不太好大刀阔斧上手）。spark也试了从3.0.0到最新版中的几个版本，但是hive没有自编译，所以总会报错。</p>
<p>另外，hadoop版本也要考虑，Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.spark.SparkTask. RPC channel is closed.这个问题在YARN日志报的$GetFileInfoRequestProto无法强制转换为com.google.protobuf.Message，主要就是hadoop版本导致的。</p>
</blockquote>
<h6 id="（2）配置流程"><a href="#（2）配置流程" class="headerlink" title="（2）配置流程"></a>（2）配置流程</h6><ul>
<li><p>在hive中创建spark配置文件</p>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># spark-defaults.conf</span><br><span class="line">spark.master                             yarn</span><br><span class="line">spark.eventLog.enabled                   true</span><br><span class="line">spark.eventLog.dir              hdfs://hadoop102:8020/spark-history</span><br><span class="line">spark.executor.memory                    1g</span><br><span class="line">spark.driver.memory					     1g</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改hive-site.xml文件，更改为spark引擎</p>
  <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--Spark依赖位置（注意：端口号8020必须和namenode的端口号一致）--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>spark.yarn.jars<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:8020/spark-jars/*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">&lt;!--Hive执行引擎--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>spark<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>增加Application Master资源比例</p>
<ul>
<li>修改YARN配置文件yarn-site.xml</li>
</ul>
  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.maximum-am-resource-percent&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;0.8&lt;/value&gt;</span><br><span class="line">&lt;/property</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="10、DolphinScheduler"><a href="#10、DolphinScheduler" class="headerlink" title="10、DolphinScheduler"></a>10、DolphinScheduler</h4><p>Apache DolphinScheduler是一个分布式、易扩展的可视化DAG工作流任务调度平台。致力于解决数据处理流程中错综复杂的依赖关系，使调度系统在数据处理流程中开箱即用。</p>
<h6 id="（1）核心架构"><a href="#（1）核心架构" class="headerlink" title="（1）核心架构"></a>（1）核心架构</h6><ul>
<li><p><strong>MasterServer</strong>采用分布式无中心设计理念，MasterServer主要负责 DAG 任务切分、任务提交、任务监控，并同时监听其它MasterServer和WorkerServer的健康状态。</p>
</li>
<li><p><strong>WorkerServer</strong>也采用分布式无中心设计理念，WorkerServer主要负责任务的执行和提供日志服务。</p>
</li>
<li><p><strong>ZooKeeper</strong>服务，系统中的MasterServer和WorkerServer节点都通过ZooKeeper来进行集群管理和容错。</p>
</li>
<li><p><strong>Alert</strong>服务，提供告警相关服务。</p>
</li>
<li><p><strong>API</strong>接口层，主要负责处理前端UI层的请求。</p>
</li>
<li><p><strong>UI</strong>，系统的前端页面，提供系统的各种可视化操作界面。</p>
</li>
</ul>
<h6 id="（2）前置准备"><a href="#（2）前置准备" class="headerlink" title="（2）前置准备"></a>（2）前置准备</h6><ul>
<li><p>三台节点均需部署JDK（1.8+），并配置相关环境变量。</p>
</li>
<li><p>需部署数据库，支持MySQL（5.7+）或者PostgreSQL（8.2.15+）。如 MySQL 则需要 JDBC Driver 8.0.16。</p>
</li>
<li><p>需部署Zookeeper（3.4.6+）。</p>
</li>
<li><p>如果启用 HDFS 文件系统，则需要 Hadoop（2.6+）环境。</p>
</li>
<li><p>三台节点均需安装进程管理工具包psmisc。</p>
</li>
</ul>
<h6 id="（3）配置流程"><a href="#（3）配置流程" class="headerlink" title="（3）配置流程"></a>（3）配置流程</h6><ul>
<li><p>配置一键部署脚本</p>
<ul>
<li><p>修改解压目录下的conf&#x2F;config目录下的install_config.conf文件</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">ips=&quot;hadoop102,hadoop103,hadoop104&quot; </span><br><span class="line"># 将要部署任一 DolphinScheduler 服务的服务器主机名或 ip 列表</span><br><span class="line"></span><br><span class="line">masters=&quot;hadoop102&quot; </span><br><span class="line"># master 所在主机名列表，必须是 ips 的子集</span><br><span class="line"></span><br><span class="line">workers=&quot;hadoop102:default,hadoop103:default,hadoop104:default&quot; </span><br><span class="line"># worker主机名及队列，此处的 ip 必须在 ips 列表中</span><br><span class="line"></span><br><span class="line">alertServer=&quot;hadoop102&quot;</span><br><span class="line"># 告警服务所在服务器主机名</span><br><span class="line"></span><br><span class="line">apiServers=&quot;hadoop102&quot;</span><br><span class="line"># api服务所在服务器主机名</span><br><span class="line"></span><br><span class="line">installPath=&quot;/opt/module/dolphinscheduler&quot;</span><br><span class="line"># DS 安装路径，如果不存在会创建</span><br><span class="line"></span><br><span class="line">deployUser=&quot;atguigu&quot;</span><br><span class="line"># 部署用户，任务执行服务是以 sudo -u &#123;linux-user&#125; 切换不同 Linux 用户的方式来实现多租户运行作业，因此该用户必须有免密的 sudo 权限。</span><br><span class="line"></span><br><span class="line">javaHome=&quot;/opt/module/jdk&quot;</span><br><span class="line"># JAVA_HOME 路径</span><br><span class="line"></span><br><span class="line">DATABASE_TYPE=&quot;mysql&quot;</span><br><span class="line"># 数据库类型</span><br><span class="line"></span><br><span class="line">SPRING_DATASOURCE_URL=&quot;jdbc:mysql://hadoop102:3306/dolphinscheduler?useUnicode=true&amp;characterEncoding=UTF-8&quot;</span><br><span class="line"># 数据库 URL</span><br><span class="line"></span><br><span class="line">SPRING_DATASOURCE_USERNAME=&quot;dolphinscheduler&quot;</span><br><span class="line"># 数据库用户名</span><br><span class="line"></span><br><span class="line">SPRING_DATASOURCE_PASSWORD=&quot;dolphinscheduler&quot;</span><br><span class="line"># 数据库密码</span><br><span class="line"></span><br><span class="line">registryPluginName=&quot;zookeeper&quot;</span><br><span class="line"># 注册中心插件名称，DS 通过注册中心来确保集群配置的一致性</span><br><span class="line"></span><br><span class="line">registryServers=&quot;hadoop102:2181,hadoop103:2181,hadoop104:2181&quot;</span><br><span class="line"># 注册中心地址，即 Zookeeper 集群的地址</span><br><span class="line"></span><br><span class="line">registryNamespace=&quot;dolphinscheduler&quot;</span><br><span class="line"># DS 在 Zookeeper 的结点名称</span><br><span class="line"></span><br><span class="line">resourceStorageType=&quot;HDFS&quot;	</span><br><span class="line"># 资源存储类型</span><br><span class="line"></span><br><span class="line">resourceUploadPath=&quot;/dolphinscheduler&quot;</span><br><span class="line"># 资源上传路径</span><br><span class="line"></span><br><span class="line">defaultFS=&quot;hdfs://hadoop102:8020&quot;</span><br><span class="line"># 默认文件系统</span><br><span class="line"></span><br><span class="line">resourceManagerHttpAddressPort=&quot;8088&quot;</span><br><span class="line"># yarn RM http 访问端口</span><br><span class="line"></span><br><span class="line">yarnHaIps=</span><br><span class="line"># Yarn RM 高可用 ip，若未启用 RM 高可用，则将该值置空</span><br><span class="line"></span><br><span class="line">singleYarnIp=&quot;hadoop103&quot;</span><br><span class="line"># Yarn RM 主机名，若启用了 HA 或未启用 RM，保留默认值</span><br><span class="line"></span><br><span class="line">hdfsRootUser=&quot;atguigu&quot;</span><br><span class="line"># 拥有 HDFS 根目录操作权限的用户</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>初始化数据库</p>
<ul>
<li><p>创建数据库</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> DATABASE dolphinscheduler <span class="keyword">DEFAULT</span> <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8 <span class="keyword">DEFAULT</span> <span class="keyword">COLLATE</span> utf8_general_ci;</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建用户</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">&#x27;dolphinscheduler&#x27;</span>@<span class="string">&#x27;%&#x27;</span> IDENTIFIED <span class="keyword">BY</span> <span class="string">&#x27;dolphinscheduler&#x27;</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>赋予用户相应权限</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">GRANT</span> <span class="keyword">ALL</span> PRIVILEGES <span class="keyword">ON</span> dolphinscheduler.<span class="operator">*</span> <span class="keyword">TO</span> <span class="string">&#x27;dolphinscheduler&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>
</li>
<li><p>拷贝MySQL驱动到DolphinScheduler的解压目录下的lib中</p>
</li>
<li><p>执行数据库初始化脚本</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">script/create-dolphinscheduler.sh</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h6 id="（3）常见启停脚本"><a href="#（3）常见启停脚本" class="headerlink" title="（3）常见启停脚本"></a>（3）常见启停脚本</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">./install.sh  # 一键部署并启动</span><br><span class="line">./bin/start-all.sh  # 启动所有服务（区别于Hadoop）</span><br><span class="line">./bin/stop-all.sh  # 关闭所有服务（区别于Hadoop）</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启停Master</span></span><br><span class="line">./bin/dolphinscheduler-daemon.sh start master-server</span><br><span class="line">./bin/dolphinscheduler-daemon.sh stop master-server</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启停Worker</span></span><br><span class="line">./bin/dolphinscheduler-daemon.sh start worker-server</span><br><span class="line">./bin/dolphinscheduler-daemon.sh stop worker-server</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启停Api</span></span><br><span class="line">./bin/dolphinscheduler-daemon.sh start api-server</span><br><span class="line">./bin/dolphinscheduler-daemon.sh stop api-server</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动Logger</span></span><br><span class="line">./bin/dolphinscheduler-daemon.sh start logger-server</span><br><span class="line">./bin/dolphinscheduler-daemon.sh stop logger-server</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启停Alert</span></span><br><span class="line">./bin/dolphinscheduler-daemon.sh start alert-server</span><br><span class="line">./bin/dolphinscheduler-daemon.sh stop alert-server</span><br></pre></td></tr></table></figure>

<blockquote>
<p>对于每个节点的配置在部署后的目录中，针对每个进程都有一个conf文件</p>
</blockquote>
<h4 id="11、Superset"><a href="#11、Superset" class="headerlink" title="11、Superset"></a>11、Superset</h4><blockquote>
<p>强烈斥责Superset的开发人员，兼容性屌差，我曾经尝试过用Docker、Miniconda安装过Superset，但是安装成功的次数寥寥无几，所以在这里着重介绍下。尤其我大一的时候，那时候B站UP主鱼皮还介绍过十分钟搭建Superset，结果放到现在这种方式不太行了（看着官方文档还是可以，但是那个依赖版本维护极差，对小白其实很不友好，尤其当时我还没学数据库），现在Superset最新版都2.0.0+了，不胜唏嘘啊。</p>
</blockquote>
<h6 id="（1）安装流程"><a href="#（1）安装流程" class="headerlink" title="（1）安装流程"></a>（1）安装流程</h6><ul>
<li><p>安装Miniconda</p>
</li>
<li><p>创建虚拟环境</p>
<ul>
<li>配置conda国内镜像</li>
<li>创建<strong>Python3.8</strong>环境</li>
</ul>
</li>
<li><p>安装依赖</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y gcc gcc-c++ libffi-devel python-devel python-pip python-wheel python-setuptools openssl-devel cyrus-sasl-devel openldap-devel</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装（更新）setuptools和pip</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --upgrade setuptools pip -i https://pypi.douban.com/simple/</span><br></pre></td></tr></table></figure>
</li>
<li><p>指定安装版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install apache-superset==1.4.2 -i https://pypi.douban.com/simple/</span><br></pre></td></tr></table></figure>
</li>
<li><p>依赖降级：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip uninstall werkzeyg</span><br><span class="line">pip uninstall markupsafe</span><br><span class="line">pip install markupsafe==2.0.1</span><br><span class="line">pip install flask==1.1.2</span><br><span class="line">pip install Werkzeug==1.0.1</span><br><span class="line">pip install --upgrade cryptography==3.2</span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化数据库之前先执行：export FLASK_APP&#x3D;superset</p>
</li>
<li><p>初始化Superset</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">superset db upgrade</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装gunicorn</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gunicorn -i https://pypi.douban.com/simple/</span><br></pre></td></tr></table></figure></li>
</ul>
<h6 id="（2）常见启停脚本-3"><a href="#（2）常见启停脚本-3" class="headerlink" title="（2）常见启停脚本"></a>（2）常见启停脚本</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gunicorn --workers 5 --timeout 120 --bind hadoop102:8787  &quot;superset.app:create_app()&quot; --daemon</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">启动Superset</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">workers：指定进程个数</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">timeout</span>：worker进程超时时间，超时会自动重启</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">bind</span>：绑定本机地址，即为Superset访问地址</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">daemon：后台运行</span></span><br></pre></td></tr></table></figure>

<h6 id="（3）安装MySQL依赖项"><a href="#（3）安装MySQL依赖项" class="headerlink" title="（3）安装MySQL依赖项"></a>（3）安装MySQL依赖项</h6><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install mysqlclient</span><br></pre></td></tr></table></figure>

<p><strong>对接不同的数据源，需安装不同的依赖，以下地址为官网说明。</strong></p>
<p><a target="_blank" rel="noopener" href="https://superset.apache.org/docs/databases/installing-database-drivers">https://superset.apache.org/docs/databases/installing-database-drivers</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://www.fenrisx.icu">Alexie-Z-Yevich</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.fenrisx.icu/2023/03/29/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E6%80%BB%E7%BB%93%E4%B8%8A/">https://www.fenrisx.icu/2023/03/29/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E6%80%BB%E7%BB%93%E4%B8%8A/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.fenrisx.icu" target="_blank">New Try && New Life</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%BB%84%E4%BB%B6/">大数据组件</a><a class="post-meta__tags" href="/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库</a></div><div class="post_share"><div class="social-share" data-image="/images/head.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2023/03/30/%E7%A6%BB%E7%BA%BF%E6%95%B0%E4%BB%93%E6%80%BB%E7%BB%93%E4%B8%8B/" title="离线数仓总结（下）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-30</div><div class="title">离线数仓总结（下）</div></div></a></div><div><a href="/2023/04/15/Flume%E4%BB%8E0%E5%88%B01/" title="Flume从0到1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-15</div><div class="title">Flume从0到1</div></div></a></div><div><a href="/2023/04/19/Spark%E4%BB%8E0%E5%88%B01%EF%BC%88%E4%B8%8A%EF%BC%89/" title="Spark从0到1（上）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-19</div><div class="title">Spark从0到1（上）</div></div></a></div><div><a href="/2023/04/20/Spark%E4%BB%8E0%E5%88%B01%EF%BC%88%E4%B8%8B%EF%BC%89/" title="Spark从0到1（下）"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-20</div><div class="title">Spark从0到1（下）</div></div></a></div><div><a href="/2023/05/12/%E5%95%86%E5%8A%A1%E6%99%BA%E8%83%BD/" title="商务智能"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-12</div><div class="title">商务智能</div></div></a></div><div><a href="/2023/03/20/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A8%E3%80%8A%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E8%B7%AF%EF%BC%9A%E9%98%BF%E9%87%8C%E5%B7%B4%E5%B7%B4%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E8%B7%B5%E3%80%8B/" title="大数据入门《大数据之路：阿里巴巴大数据实践》"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-20</div><div class="title">大数据入门《大数据之路：阿里巴巴大数据实践》</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Alexie-Z-Yevich</div><div class="author-info__description">hello</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">24</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">36</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Alexie-Z-Yevich"><i></i><span>前往我的Github...</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Alexie-Z-Yevich" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1213791406@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><center>主域名：<br><a href="https://www.fenrisx.icu"><b><font color="#5ea6e5">fenrisx.icu</font></b></a><br>其他项目：<br><a target="_blank" rel="noopener" href="http://www.fenrisx.top"><b><font color="#5ea6e5">fenrisx.top</font></b></a><br>📬：<b><font color="#a591e0">1213791406@qq.com</font></b></a></center></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E6%A6%82%E5%BF%B5%E5%8F%8A%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B"><span class="toc-text">第一章 数据仓库的概念及架构选型</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E7%9A%84%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE"><span class="toc-text">1、数据仓库的输入数据</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE"><span class="toc-text">业务数据</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE"><span class="toc-text">用户行为数据</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E6%95%B0%E6%8D%AE"><span class="toc-text">爬虫数据</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81%E9%A1%B9%E7%9B%AE%E6%A1%86%E6%9E%B6"><span class="toc-text">2、项目框架</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81%E6%B5%8B%E8%AF%95%E9%9B%86%E7%BE%A4%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%A7%84%E5%88%92"><span class="toc-text">3、测试集群服务器规划</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E7%BB%84%E4%BB%B6%E5%8F%8A%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="toc-text">第二章 组件及常见问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0%E3%80%81%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="toc-text">0、虚拟机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0-1%E3%80%81Jdk"><span class="toc-text">0.1、Jdk</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0-2%E3%80%81Mock%E6%A8%A1%E6%8B%9F%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-text">0.2、Mock模拟数据准备</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E7%94%9F%E6%88%90%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE"><span class="toc-text">（1）生成日志数据</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E7%94%9F%E6%88%90%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE"><span class="toc-text">（2）生成业务数据</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#0-3%E3%80%81Source%E7%8E%AF%E5%A2%83%E6%80%BB%E8%A7%88"><span class="toc-text">0.3、Source环境总览</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1%E3%80%81Hadoop"><span class="toc-text">1、Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B%EF%BC%9A"><span class="toc-text">（1）配置流程：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%B8%B8%E8%A7%81%E5%90%AF%E5%81%9C%E5%91%BD%E4%BB%A4"><span class="toc-text">（2）常见启停命令</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%8F%82%E6%95%B0%E8%B0%83%E4%BC%98"><span class="toc-text">（3）参数调优</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%E3%80%81Zookeeper"><span class="toc-text">2、Zookeeper</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B"><span class="toc-text">（1）配置流程</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%B8%B8%E8%A7%81%E5%90%AF%E5%81%9C%E8%84%9A%E6%9C%AC"><span class="toc-text">（2）常见启停脚本</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%E3%80%81Kafka"><span class="toc-text">3、Kafka</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B-1"><span class="toc-text">（1）配置流程</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%B8%B8%E8%A7%81%E5%90%AF%E5%81%9C%E8%84%9A%E6%9C%AC-1"><span class="toc-text">（2）常见启停脚本</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="toc-text">（3）常见命令行操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%E3%80%81Flume"><span class="toc-text">4、Flume</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%95%B4%E4%BD%93%E6%B5%81%E7%A8%8B"><span class="toc-text">（1）整体流程</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E9%85%8D%E7%BD%AE%E5%AE%9E%E6%93%8D"><span class="toc-text">（2）配置实操</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5%E3%80%81MySQL"><span class="toc-text">5、MySQL</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B"><span class="toc-text">配置流程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6%E3%80%81Maxwell"><span class="toc-text">6、Maxwell</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B-2"><span class="toc-text">（1）配置流程</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%B8%B8%E8%A7%81%E5%90%AF%E5%81%9C%E8%84%9A%E6%9C%AC-2"><span class="toc-text">（2）常见启停脚本</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%A2%9E%E9%87%8F%E5%90%8C%E6%AD%A5%E5%92%8C%E5%85%A8%E9%87%8F%E5%90%8C%E6%AD%A5"><span class="toc-text">（3）增量同步和全量同步</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#7%E3%80%81DataX"><span class="toc-text">7、DataX</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B-3"><span class="toc-text">（1）配置流程</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E"><span class="toc-text">（2）配置说明</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%883%EF%BC%89DataX%E4%BC%98%E5%8C%96"><span class="toc-text">（3）DataX优化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#8%E3%80%81Hive"><span class="toc-text">8、Hive</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B-1"><span class="toc-text">配置流程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#9%E3%80%81Spark"><span class="toc-text">9、Spark</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%85%BC%E5%AE%B9%E6%80%A7%E8%AF%B4%E6%98%8E"><span class="toc-text">（1）兼容性说明</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B"><span class="toc-text">（2）配置流程</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#10%E3%80%81DolphinScheduler"><span class="toc-text">10、DolphinScheduler</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%A0%B8%E5%BF%83%E6%9E%B6%E6%9E%84"><span class="toc-text">（1）核心架构</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%89%8D%E7%BD%AE%E5%87%86%E5%A4%87"><span class="toc-text">（2）前置准备</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E9%85%8D%E7%BD%AE%E6%B5%81%E7%A8%8B"><span class="toc-text">（3）配置流程</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%B8%B8%E8%A7%81%E5%90%AF%E5%81%9C%E8%84%9A%E6%9C%AC"><span class="toc-text">（3）常见启停脚本</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#11%E3%80%81Superset"><span class="toc-text">11、Superset</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%AE%89%E8%A3%85%E6%B5%81%E7%A8%8B"><span class="toc-text">（1）安装流程</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%B8%B8%E8%A7%81%E5%90%AF%E5%81%9C%E8%84%9A%E6%9C%AC-3"><span class="toc-text">（2）常见启停脚本</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%AE%89%E8%A3%85MySQL%E4%BE%9D%E8%B5%96%E9%A1%B9"><span class="toc-text">（3）安装MySQL依赖项</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background: rgba(255,255,255,0.7)"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By Alexie-Z-Yevich</div><div class="footer_custom_text">I wish you to become your own sun, no need to rely on who's light.<p><a target="_blank" href="https://hexo.io/"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://butterfly.js.org/"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://www.jsdelivr.com/"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&logo=jsDelivr" title="本站使用JsDelivr为静态资源提供CDN加速"></a> &nbsp;<a target="_blank" href="https://vercel.com/ "><img src="https://img.shields.io/badge/Hosted-Vervel-brightgreen?style=flat&logo=Vercel" title="本站采用双线部署，默认线路托管于Vercel"></a>&nbsp;<a target="_blank" href="https://vercel.com/ "><img src="https://img.shields.io/badge/Hosted-Coding-0cedbe?style=flat&logo=Codio" title="本站采用双线部署，联通线路托管于Coding"></a>&nbsp;<a target="_blank" href="https://github.com/"><img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async src="/js/title.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></body></html>